#!/usr/bin/env python3
"""
æ•°æ®èåˆéªŒè¯è„šæœ¬
éªŒè¯é¢„å¤„ç†æ•°æ®æ˜¯å¦æ­£ç¡®èåˆäº†G2å’ŒG3æ•°æ®é›†
"""

import json
import torch
from pathlib import Path
from collections import defaultdict, Counter
import sys
import os

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / "src"))

def load_json_data(file_path):
    """åŠ è½½JSONæ•°æ®æ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return []

def analyze_raw_data(data_dir):
    """åˆ†æåŸå§‹G2å’ŒG3æ•°æ®"""
    print("=" * 60)
    print("ğŸ“Š åŸå§‹æ•°æ®åˆ†æ")
    print("=" * 60)
    
    g2_path = data_dir / "G2_query.json"
    g3_path = data_dir / "G3_query.json"
    
    results = {}
    
    # åˆ†æG2æ•°æ®
    if g2_path.exists():
        g2_data = load_json_data(g2_path)
        g2_tools = set()
        g2_queries = []
        
        for sample in g2_data:
            if 'query' in sample:
                g2_queries.append(sample['query'])
            if 'api_list' in sample:
                for api in sample['api_list']:
                    tool_id = f"{api['tool_name']}::{api['api_name']}"
                    g2_tools.add(tool_id)
        
        results['G2'] = {
            'samples': len(g2_data),
            'unique_tools': len(g2_tools),
            'queries': len(g2_queries),
            'tools': g2_tools
        }
        
        print(f"ğŸ“ G2æ•°æ®:")
        print(f"   æ ·æœ¬æ•°é‡: {len(g2_data)}")
        print(f"   å”¯ä¸€å·¥å…·: {len(g2_tools)}")
        print(f"   æŸ¥è¯¢æ•°é‡: {len(g2_queries)}")
    else:
        print(f"âš ï¸  G2æ–‡ä»¶ä¸å­˜åœ¨: {g2_path}")
        results['G2'] = None
    
    # åˆ†æG3æ•°æ®
    if g3_path.exists():
        g3_data = load_json_data(g3_path)
        g3_tools = set()
        g3_queries = []
        
        for sample in g3_data:
            if 'query' in sample:
                g3_queries.append(sample['query'])
            if 'api_list' in sample:
                for api in sample['api_list']:
                    tool_id = f"{api['tool_name']}::{api['api_name']}"
                    g3_tools.add(tool_id)
        
        results['G3'] = {
            'samples': len(g3_data),
            'unique_tools': len(g3_tools),
            'queries': len(g3_queries),
            'tools': g3_tools
        }
        
        print(f"ğŸ“ G3æ•°æ®:")
        print(f"   æ ·æœ¬æ•°é‡: {len(g3_data)}")
        print(f"   å”¯ä¸€å·¥å…·: {len(g3_tools)}")
        print(f"   æŸ¥è¯¢æ•°é‡: {len(g3_queries)}")
    else:
        print(f"âš ï¸  G3æ–‡ä»¶ä¸å­˜åœ¨: {g3_path}")
        results['G3'] = None
    
    # è®¡ç®—é‡å å’Œæ€»è®¡
    if results['G2'] and results['G3']:
        common_tools = results['G2']['tools'] & results['G3']['tools']
        total_unique_tools = results['G2']['tools'] | results['G3']['tools']
        
        print(f"\nğŸ”— æ•°æ®é‡å åˆ†æ:")
        print(f"   G2ç‹¬æœ‰å·¥å…·: {len(results['G2']['tools'] - results['G3']['tools'])}")
        print(f"   G3ç‹¬æœ‰å·¥å…·: {len(results['G3']['tools'] - results['G2']['tools'])}")
        print(f"   å…±åŒå·¥å…·: {len(common_tools)}")
        print(f"   æ€»å”¯ä¸€å·¥å…·: {len(total_unique_tools)}")
        print(f"   æ€»æ ·æœ¬æ•°: {results['G2']['samples'] + results['G3']['samples']}")
        
        results['overlap'] = {
            'common_tools': len(common_tools),
            'total_unique_tools': len(total_unique_tools),
            'total_samples': results['G2']['samples'] + results['G3']['samples']
        }
    
    return results

def analyze_preprocessed_data(preprocessed_dir):
    """åˆ†æé¢„å¤„ç†æ•°æ®"""
    print("\n" + "=" * 60)
    print("ğŸ”§ é¢„å¤„ç†æ•°æ®åˆ†æ")
    print("=" * 60)
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    files_to_check = [
        'full_graph.pt',
        'training_samples.pt',
        'validation_samples.pt', 
        'test_samples.pt'
    ]
    
    missing_files = []
    for file_name in files_to_check:
        file_path = preprocessed_dir / file_name
        if not file_path.exists():
            missing_files.append(file_name)
    
    if missing_files:
        print(f"âŒ ç¼ºå°‘é¢„å¤„ç†æ–‡ä»¶: {missing_files}")
        return None
    
    results = {}
    
    # åˆ†æå®Œæ•´å›¾æ•°æ®
    try:
        full_graph = torch.load(preprocessed_dir / 'full_graph.pt', weights_only=True)
        tool_to_idx = full_graph['tool_to_idx']
        edge_index = full_graph['edge_index']
        node_features = full_graph['node_features']
        
        results['graph'] = {
            'num_tools': len(tool_to_idx),
            'num_edges': edge_index.size(1),
            'node_feature_dim': node_features.size(1),
            'tools': set(tool_to_idx.keys())
        }
        
        print(f"ğŸ“Š å®Œæ•´å›¾æ•°æ®:")
        print(f"   å·¥å…·èŠ‚ç‚¹æ•°: {len(tool_to_idx)}")
        print(f"   è¾¹æ•°é‡: {edge_index.size(1)}")
        print(f"   èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {node_features.size(1)}")
        
    except Exception as e:
        print(f"âŒ åŠ è½½å®Œæ•´å›¾æ•°æ®å¤±è´¥: {e}")
        return None
    
    # åˆ†ææ ·æœ¬æ•°æ®
    sample_files = ['training_samples.pt', 'validation_samples.pt', 'test_samples.pt']
    sample_names = ['è®­ç»ƒ', 'éªŒè¯', 'æµ‹è¯•']
    
    total_samples = 0
    for file_name, name in zip(sample_files, sample_names):
        try:
            samples = torch.load(preprocessed_dir / file_name, weights_only=True)
            sample_count = len(samples)
            total_samples += sample_count
            
            results[name] = {
                'count': sample_count,
                'samples': samples
            }
            
            print(f"   {name}æ ·æœ¬: {sample_count}")
            
        except Exception as e:
            print(f"âŒ åŠ è½½{name}æ ·æœ¬å¤±è´¥: {e}")
            return None
    
    results['total_samples'] = total_samples
    print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
    
    return results

def verify_data_consistency(raw_results, preprocessed_results):
    """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
    print("\n" + "=" * 60)
    print("âœ… æ•°æ®ä¸€è‡´æ€§éªŒè¯")
    print("=" * 60)
    
    if not raw_results or not preprocessed_results:
        print("âŒ æ— æ³•è¿›è¡Œä¸€è‡´æ€§éªŒè¯ï¼Œç¼ºå°‘å¿…è¦æ•°æ®")
        return False
    
    success = True
    
    # éªŒè¯å·¥å…·æ•°é‡
    if 'overlap' in raw_results:
        expected_tools = raw_results['overlap']['total_unique_tools']
        actual_tools = preprocessed_results['graph']['num_tools']
        
        print(f"ğŸ”§ å·¥å…·æ•°é‡éªŒè¯:")
        print(f"   æœŸæœ›å·¥å…·æ•°: {expected_tools}")
        print(f"   å®é™…å·¥å…·æ•°: {actual_tools}")
        
        if expected_tools == actual_tools:
            print("   âœ… å·¥å…·æ•°é‡ä¸€è‡´")
        else:
            print("   âŒ å·¥å…·æ•°é‡ä¸ä¸€è‡´")
            success = False
    
    # éªŒè¯æ ·æœ¬æ•°é‡
    if 'overlap' in raw_results:
        expected_samples = raw_results['overlap']['total_samples']
        actual_samples = preprocessed_results['total_samples']
        
        print(f"\nğŸ“Š æ ·æœ¬æ•°é‡éªŒè¯:")
        print(f"   æœŸæœ›æ ·æœ¬æ•°: {expected_samples}")
        print(f"   å®é™…æ ·æœ¬æ•°: {actual_samples}")
        
        # ç”±äºæ•°æ®é¢„å¤„ç†å¯èƒ½ä¼šè¿‡æ»¤æ‰ä¸€äº›æ— æ•ˆæ ·æœ¬ï¼Œå®é™…æ ·æœ¬æ•°å¯èƒ½å°äºç­‰äºæœŸæœ›æ ·æœ¬æ•°
        if actual_samples <= expected_samples:
            print("   âœ… æ ·æœ¬æ•°é‡åˆç†ï¼ˆé¢„å¤„ç†å¯èƒ½è¿‡æ»¤äº†æ— æ•ˆæ ·æœ¬ï¼‰")
            if actual_samples < expected_samples:
                filtered = expected_samples - actual_samples
                print(f"   â„¹ï¸  è¿‡æ»¤äº† {filtered} ä¸ªæ ·æœ¬ ({filtered/expected_samples*100:.1f}%)")
        else:
            print("   âŒ æ ·æœ¬æ•°é‡å¼‚å¸¸ï¼ˆå®é™…å¤§äºæœŸæœ›ï¼‰")
            success = False
    
    # éªŒè¯å·¥å…·è¦†ç›–
    if raw_results['G2'] and raw_results['G3']:
        raw_tools = raw_results['G2']['tools'] | raw_results['G3']['tools']
        preprocessed_tools = preprocessed_results['graph']['tools']
        
        print(f"\nğŸ”— å·¥å…·è¦†ç›–éªŒè¯:")
        missing_tools = raw_tools - preprocessed_tools
        extra_tools = preprocessed_tools - raw_tools
        
        if not missing_tools and not extra_tools:
            print("   âœ… å·¥å…·é›†å®Œå…¨ä¸€è‡´")
        else:
            if missing_tools:
                print(f"   âš ï¸  ç¼ºå°‘å·¥å…·: {len(missing_tools)} ä¸ª")
                if len(missing_tools) <= 5:
                    for tool in list(missing_tools)[:5]:
                        print(f"      - {tool}")
            if extra_tools:
                print(f"   âš ï¸  é¢å¤–å·¥å…·: {len(extra_tools)} ä¸ª")
                if len(extra_tools) <= 5:
                    for tool in list(extra_tools)[:5]:
                        print(f"      - {tool}")
    
    return success

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” GraphTool æ•°æ®èåˆéªŒè¯")
    print("éªŒè¯é¢„å¤„ç†æ•°æ®æ˜¯å¦æ­£ç¡®èåˆäº†G2å’ŒG3æ•°æ®é›†")
    
    # è®¾ç½®è·¯å¾„
    project_root = Path(__file__).parent.parent
    data_dir = project_root / "datasets" / "ToolBench"
    preprocessed_dir = data_dir / "preprocessed_data"
    
    print(f"\nğŸ“‚ æ•°æ®ç›®å½•: {data_dir}")
    print(f"ğŸ“‚ é¢„å¤„ç†ç›®å½•: {preprocessed_dir}")
    
    # åˆ†æåŸå§‹æ•°æ®
    raw_results = analyze_raw_data(data_dir)
    
    # åˆ†æé¢„å¤„ç†æ•°æ®
    preprocessed_results = analyze_preprocessed_data(preprocessed_dir)
    
    # éªŒè¯ä¸€è‡´æ€§
    is_consistent = verify_data_consistency(raw_results, preprocessed_results)
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“‹ éªŒè¯æ€»ç»“")
    print("=" * 60)
    
    if is_consistent:
        print("âœ… æ•°æ®èåˆéªŒè¯é€šè¿‡ï¼é¢„å¤„ç†æ•°æ®æ­£ç¡®èåˆäº†G2å’ŒG3æ•°æ®é›†ã€‚")
    else:
        print("âŒ æ•°æ®èåˆéªŒè¯å¤±è´¥ï¼è¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†è¿‡ç¨‹ã€‚")
    
    print("\nğŸ¯ å»ºè®®:")
    print("   - å¦‚æœéªŒè¯é€šè¿‡ï¼Œå¯ä»¥æ”¾å¿ƒä½¿ç”¨å½“å‰çš„é¢„å¤„ç†æ•°æ®")
    print("   - å¦‚æœéªŒè¯å¤±è´¥ï¼Œå»ºè®®é‡æ–°è¿è¡Œæ•°æ®é¢„å¤„ç†")
    print("   - å¯ä»¥åˆ é™¤ preprocessed_data ç›®å½•åé‡æ–°è¿è¡Œè®­ç»ƒæ¥é‡æ–°é¢„å¤„ç†")

if __name__ == "__main__":
    main()
